Big Data Module - Sentiment Analysis
Alican Tanaçan

---- Introduction ----
The Helio Project is about finding the most suitable, sentimental smart phone for medical apps to help aid workers communicate 
with each other and diagnose diseases by examining pictures and other data provided by the local aid worker. 
The government agency requires that the app suite be bundled with one model of smart phone and Helio is in the process of evaluating 
potential handset models to determine which one to bundle their software with. This report contains an analysis of sentiment toward 
the target devices, as well as a description of the methods and processes, which are used to arrive at our conclusions.
After completing an initial investigation on the data that were obtained from the webpages, it was found that five devices are 
capable of executing the app suite’s functions. To help narrow this list down to one device, we were asked to examine the prevalence 
of positive and negative attitudes toward iphone and Samsung galaxy devices. 
First we collected data from the common crawl, which is an open repository of web crawl data that is stored on Amazon’s public data sets. 
Using labeled small matrices, we developed machine learning models to determine sentiment automatically and applied those models on the 
large matrix we acquired from Amazon Web Services.
The results of the initial models had very low accuracy and kappa scores. However, after series of preprocessing and feature engineering efforts, 
we had achieved accurate models. The probability of the positive and negative sentiments in actual, train data is compared to the probability of 
positive and negative sentiments on the large data that we made predictions. Iphone’s sentiment is slightly better than Samsung galaxy.

The data were formed by scanning the common crawl for documents that contain meaningful words or phrases about the sentiment of phones’ key features 
such as camera, display, performance and operating system. Variables were generated by simply counting those relevant words towards phones’ features. 
To preserve relevancy among the variables, these features were divided according to the positive, negative and neutral reviews for each attribute. 
The approach of collecting information for these features were done by a python script that looks for positive, negative, and neutral words that are 
within five words of a feature word (camera, display, performance) or an alternate of a feature word (e.g., iSight).
The ultimate sentiment variables that grade the phones’ general sentiment in a 6 level scale from “very negative” to “very positive” were manually 
labeled by the analytics team. Manually labeling means that the team read through each webpage and assigned a sentiment rating based on their findings.
The iphone matrix provided us with roughly 12.000 observations and 14 variables on the other hand galaxy matrix only gave us 878 observations and 14 variables. 
Moreover the distribution among the sentiment ratings for both of the phones was not evenly distributed.

---- Modelization Process ----
We trained many models with C5.0 decision tree, random forest, SVM and kkNN algorithms on multiple subsetting, sampling and feature selection preprocesses. 
We started our journey by filtering web pages (rows) that contain at least one observation (review, word) about that phone. Later we selected that 
device’s related features and trained our models. With 14 variables our initial models had low accuracy to predict 6 categorical levels of sentiment.

The story behind our findings is that our train data sets do not have enough and equal amount of information about each sentiment class to predict 6 class levels. 
The imbalance among the sentiment rating in the train data can be solved by decreasing the class levels of our dependent variable to 3 or 2, and oversampling 
the iphone train data and undersampling the galaxy train data to increase the amount of observations for each class level. In this particular case we decreased 
the levels to 2, “Negative” for all negative sentiments and “Positive” for all positive sentiments; and we used both oversampling and undersampling methods 
to increase the amount of examples for each sentiment. 

The variable importance tables also gave us a hint about which variables to deselect. Words about ios and google android operating systems are not important 
for our models. Moreover during the iteration of model development and improvement, removing the “iphone” and “samsunggalaxy” columns, that show how many 
times the word “iphone” or “samsunggalaxy” were used in that webpage, also improved our models. (A relation between the amount of device name mentions 
in a webpage, with its sentiment rating is also not so logical.) In addition to these, removing rows, webpages with less than a few reviews for both data 
sets also provided us more reliable information about the phones’ sentiments and save us from the noise. 

---- Confidence of Predictions ----
After these preprocesses, feature selections and sampling methods we had reached more accurate models. It may be possible to further improve accuracy and 
kappa scores, and get better results on the test sets; nonetheless the process of this analysis – the approach of collecting information for the features, 
researching every webpage before labeling manually, the data mining efforts – reassure that our predictions are logical. It is indeed a close call when 
you put Apple iphone and Samsung galaxy head to head. The rapid technological development of each model of these brands is improving their performance, 
camera and display.
Even though we had very few observations in galaxy data set, our predictions brought realistic results about these top of the line phones. 
Both of the devices’ positive sentiment ratios are almost equal.